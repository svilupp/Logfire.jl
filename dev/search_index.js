var documenterSearchIndex = {"docs":
[{"location":"query-api/#Query-API","page":"Query API","title":"Query API","text":"Download your telemetry data from Logfire using SQL queries.","category":"section"},{"location":"query-api/#Setup","page":"Query API","title":"Setup","text":"","category":"section"},{"location":"query-api/#1.-Create-a-Read-Token","page":"Query API","title":"1. Create a Read Token","text":"Go to logfire.pydantic.dev\nSelect your project\nClick Settings (gear icon) → Read tokens tab\nClick \"Create read token\"\nCopy the token immediately (it won't be shown again)","category":"section"},{"location":"query-api/#2.-Configure-Environment","page":"Query API","title":"2. Configure Environment","text":"Add to your .env file:\n\nLOGFIRE_READ_TOKEN=pylf_v1_us_...\n\nOr set in Julia:\n\nENV[\"LOGFIRE_READ_TOKEN\"] = \"pylf_v1_us_...\"","category":"section"},{"location":"query-api/#Basic-Usage","page":"Query API","title":"Basic Usage","text":"using DotEnv\nDotEnv.load!()  # Load .env file (must call explicitly)\n\nusing Logfire\n\n# Create client (uses LOGFIRE_READ_TOKEN from environment)\nclient = LogfireQueryClient()\n\n# Or provide token directly:\n# client = LogfireQueryClient(read_token=\"pylf_v1_us_...\")\n\n# Query with row-oriented results (default)\nrows = query_json(client, \"SELECT span_name, duration FROM records LIMIT 10\")\n\nfor row in rows\n    println(\"$(row[\"span_name\"]): $(row[\"duration\"])s\")\nend","category":"section"},{"location":"query-api/#Response-Formats","page":"Query API","title":"Response Formats","text":"","category":"section"},{"location":"query-api/#Row-Oriented-(Default)","page":"Query API","title":"Row-Oriented (Default)","text":"Returns a Vector{Dict{String,Any}} where each element is a row:\n\nrows = query_json(client, \"SELECT span_name, duration FROM records LIMIT 3\")\n# [\n#   Dict(\"span_name\" => \"api-request\", \"duration\" => 0.123),\n#   Dict(\"span_name\" => \"db-query\", \"duration\" => 0.045),\n#   Dict(\"span_name\" => \"cache-hit\", \"duration\" => 0.002)\n# ]","category":"section"},{"location":"query-api/#Column-Oriented","page":"Query API","title":"Column-Oriented","text":"Returns a Dict{String,Vector} with column names as keys:\n\ncols = query_json(client, \"SELECT span_name, duration FROM records LIMIT 3\"; row_oriented=false)\n# Dict(\n#   \"span_name\" => [\"api-request\", \"db-query\", \"cache-hit\"],\n#   \"duration\" => [0.123, 0.045, 0.002]\n# )","category":"section"},{"location":"query-api/#CSV-Export","page":"Query API","title":"CSV Export","text":"Returns raw CSV as a string:\n\ncsv_data = query_csv(client, \"SELECT span_name, duration FROM records LIMIT 100\")\n\n# Save to file\nopen(\"export.csv\", \"w\") do f\n    write(f, csv_data)\nend","category":"section"},{"location":"query-api/#Query-Parameters","page":"Query API","title":"Query Parameters","text":"query_json(client, sql;\n    row_oriented = true,      # true for Vector{Dict}, false for Dict{String,Vector}\n    min_timestamp = nothing,  # ISO-8601 lower bound, e.g., \"2024-01-01T00:00:00Z\"\n    max_timestamp = nothing,  # ISO-8601 upper bound\n    limit = nothing           # Max rows (default: 500, max: 10000)\n)","category":"section"},{"location":"query-api/#Time-Filtered-Query","page":"Query API","title":"Time-Filtered Query","text":"using Dates\n\n# Last 24 hours\nyesterday = now(UTC) - Hour(24)\nmin_ts = Dates.format(yesterday, \"yyyy-mm-ddTHH:MM:SSZ\")\n\nrows = query_json(client, \"\"\"\n    SELECT span_name, duration\n    FROM records\n    ORDER BY start_timestamp DESC\n\"\"\"; min_timestamp=min_ts, limit=100)","category":"section"},{"location":"query-api/#EU-Region","page":"Query API","title":"EU Region","text":"For EU-hosted projects, use the EU endpoint:\n\nclient = LogfireQueryClient(endpoint=QUERY_ENDPOINT_EU)","category":"section"},{"location":"query-api/#Example-Queries","page":"Query API","title":"Example Queries","text":"","category":"section"},{"location":"query-api/#Most-Common-Operations","page":"Query API","title":"Most Common Operations","text":"SELECT COUNT() AS count, span_name\nFROM records\nGROUP BY span_name\nORDER BY count DESC\nLIMIT 10","category":"section"},{"location":"query-api/#Recent-Exceptions","page":"Query API","title":"Recent Exceptions","text":"SELECT exception_type, exception_message, trace_id\nFROM records\nWHERE is_exception\nORDER BY start_timestamp DESC\nLIMIT 20","category":"section"},{"location":"query-api/#P95-Latency-by-Operation","page":"Query API","title":"P95 Latency by Operation","text":"SELECT\n    span_name,\n    approx_percentile_cont(0.95) WITHIN GROUP (ORDER BY duration) as P95\nFROM records\nWHERE duration IS NOT NULL\nGROUP BY span_name\nORDER BY P95 DESC","category":"section"},{"location":"query-api/#Total-Duration-by-Operation","page":"Query API","title":"Total Duration by Operation","text":"SELECT SUM(duration) AS total_duration, span_name\nFROM records\nWHERE duration IS NOT NULL\nGROUP BY span_name\nORDER BY total_duration DESC","category":"section"},{"location":"query-api/#Slowest-Traces","page":"Query API","title":"Slowest Traces","text":"SELECT trace_id, duration, message\nFROM records\nORDER BY duration DESC\nLIMIT 10","category":"section"},{"location":"query-api/#Time-Series-Requests-per-Minute","page":"Query API","title":"Time Series - Requests per Minute","text":"SELECT\n    date_trunc('minute', start_timestamp) AS minute,\n    COUNT() as count\nFROM records\nGROUP BY minute\nORDER BY minute DESC\nLIMIT 60","category":"section"},{"location":"query-api/#LLM-Token-Usage","page":"Query API","title":"LLM Token Usage","text":"SELECT\n    span_name,\n    SUM(CAST(attributes['gen_ai.usage.input_tokens'] AS INTEGER)) as input_tokens,\n    SUM(CAST(attributes['gen_ai.usage.output_tokens'] AS INTEGER)) as output_tokens\nFROM records\nWHERE span_name LIKE 'gen_ai%'\nGROUP BY span_name","category":"section"},{"location":"query-api/#Full-Example","page":"Query API","title":"Full Example","text":"See examples/query_api_example.jl for a complete working example.\n\nusing Logfire\nusing DotEnv\nusing Dates\n\nDotEnv.load!()\n\nclient = LogfireQueryClient()\n\n# Get recent operations\nrows = query_json(client, \"\"\"\n    SELECT span_name, duration, start_timestamp\n    FROM records\n    ORDER BY start_timestamp DESC\n    LIMIT 5\n\"\"\")\n\nprintln(\"Recent operations:\")\nfor row in rows\n    println(\"  $(row[\"span_name\"]): $(row[\"duration\"])s\")\nend\n\n# Aggregate stats\nstats = query_json(client, \"\"\"\n    SELECT COUNT() AS count, span_name\n    FROM records\n    GROUP BY span_name\n    ORDER BY count DESC\n    LIMIT 5\n\"\"\")\n\nprintln(\"\\nTop operations:\")\nfor s in stats\n    println(\"  $(s[\"span_name\"]): $(s[\"count\"]) occurrences\")\nend\n\n# Export to CSV\ncsv = query_csv(client, \"SELECT * FROM records LIMIT 1000\")\nopen(\"telemetry_export.csv\", \"w\") do f\n    write(f, csv)\nend\nprintln(\"\\nExported to telemetry_export.csv\")","category":"section"},{"location":"alternative-backends/#Alternative-Backends","page":"Alternative Backends","title":"Alternative Backends","text":"Logfire.jl uses OpenTelemetry under the hood, which means you can send telemetry to any backend that supports the OTLP protocol. This allows you to:\n\nDevelop locally without sending data to Logfire cloud\nUse self-hosted observability platforms\nIntegrate with existing infrastructure (Jaeger, Grafana, Datadog, etc.)\nAvoid vendor lock-in","category":"section"},{"location":"alternative-backends/#Environment-Variables","page":"Alternative Backends","title":"Environment Variables","text":"Configure alternative backends using standard OpenTelemetry environment variables:\n\nVariable Purpose Example\nOTEL_EXPORTER_OTLP_ENDPOINT Base URL for the OTLP endpoint http://localhost:4318\nOTEL_EXPORTER_OTLP_HEADERS Custom headers (authentication) Authorization=Bearer token\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT Traces-specific endpoint http://localhost:4318/v1/traces\nOTEL_EXPORTER_OTLP_METRICS_ENDPOINT Metrics-specific endpoint http://localhost:4318/v1/metrics\n\nNote: Data is sent using Protobuf over HTTP (not gRPC). Ensure your backend supports this format.","category":"section"},{"location":"alternative-backends/#Local-Development-with-Jaeger","page":"Alternative Backends","title":"Local Development with Jaeger","text":"Jaeger is an open-source distributed tracing platform that's perfect for local development.","category":"section"},{"location":"alternative-backends/#1.-Start-Jaeger","page":"Alternative Backends","title":"1. Start Jaeger","text":"docker run --rm \\\n  -p 16686:16686 \\\n  -p 4318:4318 \\\n  jaegertracing/all-in-one:latest\n\nPorts:\n\n16686: Jaeger UI\n4318: OTLP HTTP receiver","category":"section"},{"location":"alternative-backends/#2.-Configure-Logfire.jl","page":"Alternative Backends","title":"2. Configure Logfire.jl","text":"using Logfire\n\nENV[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://localhost:4318\"\n\nLogfire.configure(\n    service_name = \"my-julia-app\",\n    send_to_logfire = :always  # Force export even without Logfire token\n)","category":"section"},{"location":"alternative-backends/#3.-Create-Traces","page":"Alternative Backends","title":"3. Create Traces","text":"with_span(\"main-operation\") do\n    println(\"Doing work...\")\n\n    with_span(\"sub-task-1\") do\n        sleep(0.1)\n    end\n\n    with_span(\"sub-task-2\") do\n        sleep(0.2)\n    end\nend\n\nLogfire.flush!()","category":"section"},{"location":"alternative-backends/#4.-View-in-Jaeger","page":"Alternative Backends","title":"4. View in Jaeger","text":"Open http://localhost:16686 and select your service from the dropdown.","category":"section"},{"location":"alternative-backends/#Using-with-Langfuse","page":"Alternative Backends","title":"Using with Langfuse","text":"Langfuse is an open-source LLM observability platform with OTEL support.","category":"section"},{"location":"alternative-backends/#Cloud-(langfuse.com)","page":"Alternative Backends","title":"Cloud (langfuse.com)","text":"using Logfire\n\n# Get your credentials from Langfuse dashboard\nENV[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://cloud.langfuse.com/api/public/otel\"\nENV[\"OTEL_EXPORTER_OTLP_HEADERS\"] = \"Authorization=Basic $(base64encode(\"public-key:secret-key\"))\"\n\nLogfire.configure(\n    service_name = \"my-llm-app\",\n    send_to_logfire = :always\n)","category":"section"},{"location":"alternative-backends/#Self-Hosted-Langfuse","page":"Alternative Backends","title":"Self-Hosted Langfuse","text":"ENV[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://localhost:3000/api/public/otel\"\nENV[\"OTEL_EXPORTER_OTLP_HEADERS\"] = \"Authorization=Basic $(base64encode(\"pk-...:sk-...\"))\"","category":"section"},{"location":"alternative-backends/#OpenTelemetry-Collector","page":"Alternative Backends","title":"OpenTelemetry Collector","text":"The OpenTelemetry Collector is a vendor-agnostic proxy that can receive, process, and export telemetry data to multiple backends.","category":"section"},{"location":"alternative-backends/#1.-Create-Collector-Config","page":"Alternative Backends","title":"1. Create Collector Config","text":"# otel-collector-config.yaml\nreceivers:\n  otlp:\n    protocols:\n      http:\n        endpoint: 0.0.0.0:4318\n\nexporters:\n  debug:\n    verbosity: detailed\n  # Add your backends here:\n  # jaeger:\n  #   endpoint: jaeger:14250\n  # prometheus:\n  #   endpoint: \"0.0.0.0:8889\"\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [debug]\n    metrics:\n      receivers: [otlp]\n      exporters: [debug]","category":"section"},{"location":"alternative-backends/#2.-Run-Collector","page":"Alternative Backends","title":"2. Run Collector","text":"docker run --rm \\\n  -p 4318:4318 \\\n  -v $(pwd)/otel-collector-config.yaml:/etc/otelcol/config.yaml \\\n  otel/opentelemetry-collector:latest","category":"section"},{"location":"alternative-backends/#3.-Configure-Logfire.jl","page":"Alternative Backends","title":"3. Configure Logfire.jl","text":"ENV[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://localhost:4318\"\nLogfire.configure(service_name = \"my-app\", send_to_logfire = :always)","category":"section"},{"location":"alternative-backends/#Grafana-Cloud","page":"Alternative Backends","title":"Grafana Cloud","text":"ENV[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://otlp-gateway-prod-us-central-0.grafana.net/otlp\"\nENV[\"OTEL_EXPORTER_OTLP_HEADERS\"] = \"Authorization=Basic $(base64encode(\"instance-id:api-key\"))\"\n\nLogfire.configure(service_name = \"my-app\", send_to_logfire = :always)","category":"section"},{"location":"alternative-backends/#Dual-Export-(Logfire-Local)","page":"Alternative Backends","title":"Dual Export (Logfire + Local)","text":"To send data to both Logfire cloud and a local collector, use an OpenTelemetry Collector as a fanout proxy:\n\n# otel-collector-config.yaml\nreceivers:\n  otlp:\n    protocols:\n      http:\n        endpoint: 0.0.0.0:4318\n\nexporters:\n  otlphttp/logfire:\n    endpoint: https://logfire-us.pydantic.dev\n    headers:\n      Authorization: \"Bearer ${LOGFIRE_TOKEN}\"\n  otlphttp/jaeger:\n    endpoint: http://jaeger:4318\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [otlphttp/logfire, otlphttp/jaeger]\n\nThen point your Julia app at the collector:\n\nENV[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"http://localhost:4318\"\nLogfire.configure(service_name = \"my-app\", send_to_logfire = :always)","category":"section"},{"location":"alternative-backends/#Disabling-Cloud-Export","page":"Alternative Backends","title":"Disabling Cloud Export","text":"To use only local backends without any cloud export:\n\nLogfire.configure(\n    service_name = \"my-app\",\n    send_to_logfire = :never  # Never send to Logfire cloud\n)\n\nOr set the environment variable:\n\nLOGFIRE_SEND_TO_LOGFIRE=never","category":"section"},{"location":"alternative-backends/#Troubleshooting","page":"Alternative Backends","title":"Troubleshooting","text":"","category":"section"},{"location":"alternative-backends/#No-traces-appearing","page":"Alternative Backends","title":"No traces appearing","text":"Check that your backend is running and accessible\nVerify the endpoint URL is correct (include /v1/traces if using specific endpoint vars)\nEnsure send_to_logfire = :always is set when no Logfire token is present\nCall Logfire.flush!() before your program exits","category":"section"},{"location":"alternative-backends/#Authentication-errors","page":"Alternative Backends","title":"Authentication errors","text":"Check that headers are formatted correctly:\n\n# Correct\nENV[\"OTEL_EXPORTER_OTLP_HEADERS\"] = \"Authorization=Bearer mytoken\"\n\n# Wrong (no spaces around =)\nENV[\"OTEL_EXPORTER_OTLP_HEADERS\"] = \"Authorization = Bearer mytoken\"","category":"section"},{"location":"alternative-backends/#Protocol-mismatch","page":"Alternative Backends","title":"Protocol mismatch","text":"Logfire.jl sends data using HTTP/Protobuf. If your backend only supports gRPC, you'll need to use an OpenTelemetry Collector as a protocol converter.","category":"section"},{"location":"#Logfire.jl","page":"Home","title":"Logfire.jl","text":"Julia client for Pydantic Logfire - OpenTelemetry-based observability for LLM applications.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"OpenTelemetry Integration - Full OTEL support for tracing LLM calls\nPromptingTools.jl Support - Automatic instrumentation of aigenerate, aitools, aiextract\nGenAI Semantic Conventions - Compliant with OTEL GenAI specs\nQuery API - Download your telemetry data using SQL\nAlternative Backends - Send data to Jaeger, Langfuse, or any OTEL-compatible backend\nException Tracking - Automatic exception capture with full stacktraces","category":"section"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"using DotEnv\nDotEnv.load!()  # Load .env file (must call explicitly)\n\nusing Logfire\nusing PromptingTools\n\n# Configure Logfire (uses LOGFIRE_TOKEN from environment)\nLogfire.configure(service_name=\"my-app\")\n\n# Instrument PromptingTools\nLogfire.instrument_promptingtools!()\n\n# All LLM calls are now traced\nresponse = aigenerate(\"What is 2+2?\")","category":"section"},{"location":"#Manual-Schema-Wrapping-(No-Auto-Instrumentation)","page":"Home","title":"Manual Schema Wrapping (No Auto-Instrumentation)","text":"If you prefer not to use auto-instrumentation, you can explicitly wrap any PromptingTools schema:\n\nusing Logfire, PromptingTools\n\nLogfire.configure(service_name = \"my-app\")\n\n# Wrap the schema you want to trace\nschema = PromptingTools.OpenAISchema() |> Logfire.LogfireSchema\n\n# Use it directly - no instrument_promptingtools!() needed\naigenerate(schema, \"Hello!\"; model = \"gpt-5-mini\")\n\nThis gives you fine-grained control over which calls are traced.","category":"section"},{"location":"#Authentication","page":"Home","title":"Authentication","text":"Set your Logfire token via one of:\n\n.env file with LOGFIRE_TOKEN=... (call DotEnv.load!() first)\nEnvironment variable: ENV[\"LOGFIRE_TOKEN\"] = \"...\"\nDirect argument: Logfire.configure(token=\"...\")","category":"section"},{"location":"#What-Gets-Captured","page":"Home","title":"What Gets Captured","text":"Request params: model, temperature, topp, maxtokens, stop, penalties\nUsage: input/output/total tokens, latency, cost\nProvider metadata: model returned, status, finishreason, responseid\nTool/function calls: count + full payload\nConversation: roles + content for all messages\nExceptions: type, message, and full stacktrace","category":"section"},{"location":"#Configuration-Options","page":"Home","title":"Configuration Options","text":"Logfire.configure(\n    token = \"...\",                    # Logfire write token (or use LOGFIRE_TOKEN env)\n    service_name = \"my-app\",          # Service name for telemetry\n    service_version = \"1.0.0\",        # Service version\n    environment = \"production\",       # Deployment environment\n    send_to_logfire = :if_token_present,  # :always, :never, or :if_token_present\n    endpoint = \"...\",                 # Custom OTLP endpoint\n    auto_record_exceptions = true     # Automatic exception capture\n)","category":"section"},{"location":"#Manual-Spans","page":"Home","title":"Manual Spans","text":"# Generic span\nwith_span(\"my-operation\") do span\n    set_span_attribute!(span, \"custom.key\", \"value\")\n    # do work...\nend\n\n# LLM-specific span\nwith_llm_span(\"chat\"; system=\"openai\", model=\"gpt-4o\") do span\n    # do LLM work...\n    record_token_usage!(span, 100, 50)\nend","category":"section"},{"location":"#Exception-Handling","page":"Home","title":"Exception Handling","text":"# Automatic (default)\nwith_span(\"risky-operation\") do span\n    error(\"Oops!\")  # Automatically captured\nend\n\n# Manual\ntry\n    risky_operation()\ncatch e\n    record_exception!(span, e; backtrace=catch_backtrace())\n    rethrow()\nend","category":"section"},{"location":"#Documentation","page":"Home","title":"Documentation","text":"Query API - Download telemetry data using SQL queries\nAlternative Backends - Use Jaeger, Langfuse, or other OTEL backends\nOpenTelemetry GenAI Semantic Conventions - Message formats and span attributes","category":"section"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"#Logfire.ERROR_TYPE_OTHER","page":"Home","title":"Logfire.ERROR_TYPE_OTHER","text":"Error type for GenAI operations.\n\nWell-known value from OTEL semantic conventions:\n\n_OTHER: Fallback error value when no custom value is defined\n\nCustom values may be used for specific error types.\n\n\n\n\n\n","category":"constant"},{"location":"#Logfire.AbstractMessagePart","page":"Home","title":"Logfire.AbstractMessagePart","text":"Abstract base type for all message parts.\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.BlobPart","page":"Home","title":"Logfire.BlobPart","text":"BlobPart\n\nRepresents blob binary data sent inline to the model.\n\nFields\n\nmodality::Modality: The general modality (image, video, audio)\ncontent::String: Base64-encoded binary content\nmime_type::Union{String,Nothing}: IANA MIME type\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.FilePart","page":"Home","title":"Logfire.FilePart","text":"FilePart\n\nRepresents an external referenced file sent to the model by file ID.\n\nFields\n\nmodality::Modality: The general modality (image, video, audio)\nfile_id::String: Identifier referencing a pre-uploaded file\nmime_type::Union{String,Nothing}: IANA MIME type\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.FinishReason","page":"Home","title":"Logfire.FinishReason","text":"Reason for finishing the generation.\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.GenericPart","page":"Home","title":"Logfire.GenericPart","text":"GenericPart\n\nRepresents an arbitrary message part with custom type. Allows extensibility with custom message part types.\n\nFields\n\ntype::String: The type identifier\nproperties::Dict{String,Any}: Additional properties\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.InputMessage","page":"Home","title":"Logfire.InputMessage","text":"InputMessage\n\nRepresents an input message sent to the model.\n\nFields\n\nrole::Role: Role of the entity that created the message\nparts::Vector{AbstractMessagePart}: List of message parts\nname::Union{String,Nothing}: Optional participant name\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.LogfireConfig","page":"Home","title":"Logfire.LogfireConfig","text":"LogfireConfig\n\nConfiguration options for Logfire SDK.\n\nFields\n\ntoken::Union{String, Nothing}: Logfire write token\nservice_name::String: Service name for telemetry\nservice_version::Union{String, Nothing}: Service version\nenvironment::String: Deployment environment (development, staging, production)\nsend_to_logfire::Symbol: Export control (:iftokenpresent, :always, :never)\nendpoint::String: OTLP endpoint URL\nconsole::Bool: Print spans to console\nscrubbing::Bool: Enable data scrubbing\nauto_record_exceptions::Bool: Automatically record exceptions in spans (default: true)\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.LogfireQueryClient","page":"Home","title":"Logfire.LogfireQueryClient","text":"LogfireQueryClient\n\nClient for querying Logfire data via the Query API.\n\nFields\n\nread_token::String: Logfire read token for authentication\nendpoint::String: Query API endpoint URL\n\nExample\n\nusing Logfire\n\n# Create client (uses LOGFIRE_READ_TOKEN from environment)\nclient = LogfireQueryClient()\n\n# Or with explicit token\nclient = LogfireQueryClient(read_token=\"pylf_v1_us_...\")\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.LogfireQueryClient-Tuple{}","page":"Home","title":"Logfire.LogfireQueryClient","text":"LogfireQueryClient(; read_token=nothing, endpoint=QUERY_ENDPOINT_US)\n\nCreate a query client for downloading data from Logfire.\n\nKeywords\n\nread_token::String: Read token (or uses LOGFIRE_READ_TOKEN env var)\nendpoint::String: Query API endpoint (default: US region)\n\nEndpoints\n\nUS: https://logfire-us.pydantic.dev/v1/query\nEU: https://logfire-eu.pydantic.dev/v1/query\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.LogfireSchema","page":"Home","title":"Logfire.LogfireSchema","text":"LogfireSchema(inner::PT.AbstractPromptSchema)\n\nTracer schema that wraps any PromptingTools prompt schema and emits OpenTelemetry GenAI spans. Works with all ai* APIs in PromptingTools.\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.Modality","page":"Home","title":"Logfire.Modality","text":"Modality of media content.\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.OperationName","page":"Home","title":"Logfire.OperationName","text":"GenAI operation type.\n\nWell-known values from OTEL semantic conventions:\n\nchat: Chat completion (e.g., OpenAI Chat API)\ncreate_agent: Create GenAI agent\nembeddings: Embeddings operation\nexecute_tool: Execute a tool\ngenerate_content: Multimodal content generation (e.g., Gemini)\ninvoke_agent: Invoke GenAI agent\ntext_completion: Text completions (legacy)\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.OutputMessage","page":"Home","title":"Logfire.OutputMessage","text":"OutputMessage\n\nRepresents an output message generated by the model.\n\nFields\n\nrole::Role: Role of the entity that created the message\nparts::Vector{AbstractMessagePart}: List of message parts\nfinish_reason::FinishReason: Reason for finishing generation\nname::Union{String,Nothing}: Optional participant name\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.OutputType","page":"Home","title":"Logfire.OutputType","text":"GenAI output type.\n\nWell-known values from OTEL semantic conventions:\n\ntext: Plain text\njson: JSON object with known or unknown schema\nimage: Image\nspeech: Speech\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.ReasoningPart","page":"Home","title":"Logfire.ReasoningPart","text":"ReasoningPart\n\nRepresents reasoning/thinking content received from the model.\n\nFields\n\ncontent::String: Reasoning/thinking content\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.Role","page":"Home","title":"Logfire.Role","text":"Role of the entity that created the message.\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.TextPart","page":"Home","title":"Logfire.TextPart","text":"TextPart\n\nRepresents text content sent to or received from the model.\n\nFields\n\ncontent::String: Text content\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.ToolCallRequestPart","page":"Home","title":"Logfire.ToolCallRequestPart","text":"ToolCallRequestPart\n\nRepresents a tool call requested by the model.\n\nFields\n\nname::String: Name of the tool\nid::Union{String,Nothing}: Unique identifier for the tool call\narguments::Any: Arguments for the tool call (Dict, String, or nothing)\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.ToolCallResponsePart","page":"Home","title":"Logfire.ToolCallResponsePart","text":"ToolCallResponsePart\n\nRepresents a tool call result sent to the model.\n\nFields\n\nresponse::Any: Tool call response\nid::Union{String,Nothing}: Unique tool call identifier\nname::Union{String,Nothing}: Name of the tool that was called\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.ToolDefinition","page":"Home","title":"Logfire.ToolDefinition","text":"ToolDefinition\n\nRepresents a tool definition in OpenAI function format.\n\nFields\n\nname::String: Tool name\ndescription::String: Tool description\nparameters::Dict{String,Any}: JSON Schema for parameters\n\n\n\n\n\n","category":"type"},{"location":"#Logfire.UriPart","page":"Home","title":"Logfire.UriPart","text":"UriPart\n\nRepresents an external referenced file sent to the model by URI.\n\nFields\n\nmodality::Modality: The general modality (image, video, audio)\nuri::String: URI referencing the data\nmime_type::Union{String,Nothing}: IANA MIME type\n\n\n\n\n\n","category":"type"},{"location":"#Logfire._extract_tool_call_data-Tuple{Any}","page":"Home","title":"Logfire._extract_tool_call_data","text":"Extract structured tool call data from a vector of tool calls. Handles both ToolMessage objects and Dict representations.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire._message_role-Tuple{Any}","page":"Home","title":"Logfire._message_role","text":"Get the role of a message, using PT's role4render when available.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire._parse_query_response-Tuple{Any, Bool}","page":"Home","title":"Logfire._parse_query_response","text":"_parse_query_response(raw, row_oriented) -> Vector{Dict} or Dict\n\nParse the Logfire API response format into user-friendly data structures.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire._record_detailed_usage!-Tuple{Any, Any}","page":"Home","title":"Logfire._record_detailed_usage!","text":"_record_detailed_usage!(span, ai_msg)\n\nRecord detailed usage statistics from extras to OTEL GenAI attributes.\n\nReads unified keys first, falls back to raw provider dicts for backwards compatibility with older PromptingTools versions.\n\nUnified keys supported:\n\n:cache_read_tokens, :cache_write_tokens - cache token usage\n:cache_write_1h_tokens, :cache_write_5m_tokens - Anthropic ephemeral cache\n:reasoning_tokens - chain-of-thought tokens\n:audio_input_tokens, :audio_output_tokens - audio tokens\n:accepted_prediction_tokens, :rejected_prediction_tokens - prediction tokens\n:service_tier - provider service tier\n:web_search_requests - Anthropic server tool usage\n\nFallback dicts:\n\n:prompt_tokens_details - OpenAI prompt token details\n:completion_tokens_details - OpenAI completion token details\n:cache_read_input_tokens, :cache_creation_input_tokens - Anthropic legacy keys\n\n\n\n\n\n","category":"method"},{"location":"#Logfire._record_messages_as_attributes!-Tuple{Any, Any}","page":"Home","title":"Logfire._record_messages_as_attributes!","text":"Record messages as span attributes using OTEL GenAI semantic conventions.\n\nSets the following attributes:\n\ngen_ai.input.messages: Chat history (all messages except final response)\ngenai.output.messages: Model response with finishreason\ngenai.systeminstructions: System prompt (extracted from conversation)\n\nUses typed constructs from types.jl for proper JSON serialization.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire._record_tool_calls!-Tuple{Any, Any}","page":"Home","title":"Logfire._record_tool_calls!","text":"Record tool calls from AIToolRequest or AIMessage.\n\nHandles both:\n\nAIToolRequest.tool_calls (direct field with Vector{ToolMessage})\nextras[:tool_calls] (fallback for AIMessage with tool calls in extras)\n\n\n\n\n\n","category":"method"},{"location":"#Logfire._setup_providers!-Tuple{Logfire.LogfireConfig}","page":"Home","title":"Logfire._setup_providers!","text":"_setup_providers!(cfg::LogfireConfig)\n\nInitialize OpenTelemetry tracer and meter providers based on configuration.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.add_prompt_attribute!-Tuple{Any, Vector}","page":"Home","title":"Logfire.add_prompt_attribute!","text":"add_prompt_attribute!(span, messages::Vector)\n\nAdd prompt messages as span attributes.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.add_response_attribute!-Tuple{Any, AbstractString}","page":"Home","title":"Logfire.add_response_attribute!","text":"add_response_attribute!(span, content::AbstractString)\n\nAdd response content as a span attribute.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.configure-Tuple{}","page":"Home","title":"Logfire.configure","text":"configure(; kwargs...)\n\nInitialize Logfire SDK with the specified options.\n\nKeywords\n\ntoken::String: Logfire write token (or use LOGFIRE_TOKEN env var)\nservice_name::String: Name of the service (default: \"julia-app\")\nservice_version::String: Version of the service\nenvironment::String: Deployment environment (default: \"development\")\nsend_to_logfire::Symbol: Export control (:iftokenpresent, :always, :never)\nendpoint::String: Custom OTLP endpoint (default: Logfire US)\nscrubbing::Bool: Enable data scrubbing (default: false)\nconsole::Bool: Print spans to console (default: false)\nauto_record_exceptions::Bool: Automatically record exceptions in spans (default: true)\n\nExample\n\nusing Logfire\n\nLogfire.configure(\n    service_name = \"my-llm-app\",\n    environment = \"production\",\n    auto_record_exceptions = true  # Automatically capture exceptions in spans\n)\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.create_logfire_exporter-Tuple{Logfire.LogfireConfig}","page":"Home","title":"Logfire.create_logfire_exporter","text":"create_logfire_exporter(cfg::LogfireConfig) -> OtlpHttpTracesExporter\n\nCreate an OTLP HTTP exporter configured for Logfire backend.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.create_resource-Tuple{Logfire.LogfireConfig}","page":"Home","title":"Logfire.create_resource","text":"create_resource(cfg::LogfireConfig) -> Resource\n\nCreate an OpenTelemetry Resource with Logfire-compatible attributes.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.flush!-Tuple{}","page":"Home","title":"Logfire.flush!","text":"flush!()\n\nForce flush any pending telemetry data. Note: With SimpleSpanProcessor, spans are exported immediately when they end. This function is mainly useful for BatchSpanProcessor, but OpenTelemetrySDK doesn't expose a direct flush API. Spans will be exported automatically.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.get_config-Tuple{}","page":"Home","title":"Logfire.get_config","text":"get_config() -> LogfireConfig\n\nGet the current global configuration.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.instrument_promptingtools!-Tuple{}","page":"Home","title":"Logfire.instrument_promptingtools!","text":"instrument_promptingtools!(; models=nothing, base_schema=PromptingTools.OpenAISchema())\n\nRegister Logfire's LogfireSchema tracer for the given model names. If models is nothing, all models currently registered in PromptingTools are instrumented. Throws an error if no models are provided and none can be discovered.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.instrument_promptingtools_model!-Tuple{Any}","page":"Home","title":"Logfire.instrument_promptingtools_model!","text":"instrument_promptingtools_model!(name; base_schema=PromptingTools.OpenAISchema())\n\nRegister Logfire tracing for a single model name or alias. Uses the already registered PromptingTools schema when available; otherwise falls back to base_schema. Safe to call multiple times.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.is_configured-Tuple{}","page":"Home","title":"Logfire.is_configured","text":"is_configured() -> Bool\n\nCheck if Logfire has been configured.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.messages_to_json-Tuple{Vector{<:InputMessage}}","page":"Home","title":"Logfire.messages_to_json","text":"messages_to_json(messages::Vector{InputMessage}) -> String\n\nSerialize input messages to JSON string for gen_ai.input.messages attribute.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.messages_to_json-Tuple{Vector{<:OutputMessage}}","page":"Home","title":"Logfire.messages_to_json","text":"messages_to_json(messages::Vector{OutputMessage}) -> String\n\nSerialize output messages to JSON string for gen_ai.output.messages attribute.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.part_to_dict-Tuple{TextPart}","page":"Home","title":"Logfire.part_to_dict","text":"Convert any message part to a Dict for JSON serialization.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.pt_conversation_to_otel-Tuple{AbstractVector}","page":"Home","title":"Logfire.pt_conversation_to_otel","text":"pt_conversation_to_otel(conv; separate_system=true) -> NamedTuple\n\nConvert a PromptingTools conversation to OTEL format.\n\nReturns (; input_messages, output_messages, system_instructions).\n\nIf separate_system=true, system messages are extracted to system_instructions.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.pt_message_to_input-Tuple{Any}","page":"Home","title":"Logfire.pt_message_to_input","text":"pt_message_to_input(msg) -> InputMessage\n\nConvert a PromptingTools message to an OTEL InputMessage.\n\nHandles:\n\nSystemMessage → role=ROLE_SYSTEM\nUserMessage → role=ROLE_USER\nUserMessageWithImages → role=ROLE_USER with BlobPart/UriPart\nAIMessage → role=ROLE_ASSISTANT\nAIToolRequest → role=ROLE_ASSISTANT with ToolCallRequestPart\nToolMessage → role=ROLE_USER with ToolCallResponsePart\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.pt_message_to_output-Tuple{Any}","page":"Home","title":"Logfire.pt_message_to_output","text":"pt_message_to_output(msg; finish_reason=nothing) -> OutputMessage\n\nConvert a PromptingTools message to an OTEL OutputMessage. Automatically detects finish_reason if not provided.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.query_csv-Tuple{LogfireQueryClient, String}","page":"Home","title":"Logfire.query_csv","text":"query_csv(client, sql; kwargs...) -> String\n\nExecute a SQL query and return CSV data as a string.\n\nArguments\n\nclient::LogfireQueryClient: Query client instance\nsql::String: SQL query to execute\n\nKeywords\n\nmin_timestamp::String: ISO-8601 lower bound for filtering\nmax_timestamp::String: ISO-8601 upper bound for filtering\nlimit::Int: Maximum rows to return (default: 500, max: 10000)\n\nReturns\n\nString: CSV-formatted data\n\nExample\n\nclient = LogfireQueryClient()\ncsv_data = query_csv(client, \"SELECT span_name, duration FROM records LIMIT 100\")\nprintln(csv_data)\n\n# Or write to file\nopen(\"export.csv\", \"w\") do f\n    write(f, csv_data)\nend\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.query_json-Tuple{LogfireQueryClient, String}","page":"Home","title":"Logfire.query_json","text":"query_json(client, sql; row_oriented=true, kwargs...) -> Vector{Dict} or Dict\n\nExecute a SQL query and return JSON data.\n\nArguments\n\nclient::LogfireQueryClient: Query client instance\nsql::String: SQL query to execute\n\nKeywords\n\nrow_oriented::Bool=true: If true, returns Vector{Dict} (each row is a dict). If false, returns Dict{String,Vector} (column-oriented)\nmin_timestamp::String: ISO-8601 lower bound for filtering (e.g., \"2024-01-01T00:00:00Z\")\nmax_timestamp::String: ISO-8601 upper bound for filtering\nlimit::Int: Maximum rows to return (default: 500, max: 10000)\n\nReturns\n\nRow-oriented (row_oriented=true): Vector{Dict} where each element is a row\nColumn-oriented (row_oriented=false): Dict{String,Vector} with column names as keys\n\nExample\n\nclient = LogfireQueryClient()\n\n# Get recent spans (row-oriented)\nrows = query_json(client, \"SELECT span_name, duration FROM records LIMIT 10\")\nfor row in rows\n    println(\"$(row[\"span_name\"]): $(row[\"duration\"])s\")\nend\n\n# Get column-oriented data\ncols = query_json(client, \"SELECT span_name, duration FROM records LIMIT 10\"; row_oriented=false)\nprintln(\"Span names: \", cols[\"span_name\"])\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.record_exception!-Tuple{Any, Any}","page":"Home","title":"Logfire.record_exception!","text":"record_exception!(span, exception; backtrace=nothing, escaped=false)\n\nRecord an exception on a span following OpenTelemetry semantic conventions.\n\nThis function sets the standard OpenTelemetry exception attributes that Logfire recognizes for its specialized exception view:\n\nexception.type: The exception type name\nexception.message: The exception message\nexception.stacktrace: The full stack trace\n\nIt also sets the span status to error and the log level to 'error'.\n\nArguments\n\nspan: The span to record the exception on\nexception: The exception object to record\nbacktrace: Optional backtrace. If not provided, attempts to use catch_backtrace()  if called within a catch block. Pass the backtrace explicitly for best results.\nescaped: Whether the exception message should be escaped (default: false, reserved for future use)\n\nExample\n\ntry\n    error(\"Something went wrong\")\ncatch e\n    bt = catch_backtrace()\n    record_exception!(span, e; backtrace=bt)\n    rethrow()\nend\n\nOr more simply, if called within the catch block:\n\ntry\n    error(\"Something went wrong\")\ncatch e\n    record_exception!(span, e)  # Will attempt to get backtrace automatically\n    rethrow()\nend\n\nThis is equivalent to Python's logfire.exception() or span.record_exception().\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.record_token_usage!-Tuple{Any, Int64, Int64}","page":"Home","title":"Logfire.record_token_usage!","text":"record_token_usage!(span, input_tokens::Int, output_tokens::Int; model::String=\"\")\n\nRecord token usage on a span following GenAI semantic conventions.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.set_genai_messages!-Tuple{Any, AbstractVector}","page":"Home","title":"Logfire.set_genai_messages!","text":"set_genai_messages!(span, conv; separate_system=true)\n\nSet genai.input.messages, genai.output.messages, and genai.systeminstructions on a span from a PromptingTools conversation.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.set_span_attribute!-Tuple{Any, String, Any}","page":"Home","title":"Logfire.set_span_attribute!","text":"set_span_attribute!(span, key::String, value)\n\nSet an attribute on a span.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.set_span_status_error!-Tuple{Any, String}","page":"Home","title":"Logfire.set_span_status_error!","text":"set_span_status_error!(span, message::String)\n\nSet span status to error with a message.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.set_tool_definitions!-Tuple{Any, AbstractDict}","page":"Home","title":"Logfire.set_tool_definitions!","text":"set_tool_definitions!(span, tool_map)\n\nSet gen_ai.tool.definitions on a span from PromptingTools tool signatures.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.set_tool_definitions!-Tuple{Any, Vector}","page":"Home","title":"Logfire.set_tool_definitions!","text":"set_tool_definitions!(span, tools::Vector)\n\nSet gen_ai.tool.definitions on a span from a vector of functions.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.should_send_to_logfire","page":"Home","title":"Logfire.should_send_to_logfire","text":"should_send_to_logfire(cfg::LogfireConfig) -> Bool\n\nDetermine if telemetry should be sent to Logfire based on configuration.\n\n\n\n\n\n","category":"function"},{"location":"#Logfire.shutdown!-Tuple{}","page":"Home","title":"Logfire.shutdown!","text":"shutdown!()\n\nGracefully shutdown the Logfire SDK, flushing any pending telemetry. Note: With SimpleSpanProcessor, spans are exported immediately when they end, so shutdown is mainly for cleanup. For BatchSpanProcessor, this would flush pending spans.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.system_instructions_to_json-Tuple{Vector{<:AbstractMessagePart}}","page":"Home","title":"Logfire.system_instructions_to_json","text":"system_instructions_to_json(parts::Vector{<:AbstractMessagePart}) -> String\n\nSerialize system instructions to JSON string for genai.systeminstructions attribute.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.tool_definitions_from_functions-Tuple{Vector}","page":"Home","title":"Logfire.tool_definitions_from_functions","text":"tool_definitions_from_functions(tools::Vector) -> Vector{ToolDefinition}\n\nCreate tool definitions from a vector of functions using PT.toolcallsignature.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.tool_definitions_from_pt-Tuple{AbstractDict}","page":"Home","title":"Logfire.tool_definitions_from_pt","text":"tool_definitions_from_pt(tool_map) -> Vector{ToolDefinition}\n\nConvert PromptingTools tool signatures to OTEL tool definitions.\n\nExample\n\nget_weather(city::String) = \"sunny\"\ntools = [get_weather]\ntool_map = PT.tool_call_signature(tools)\ndefs = tool_definitions_from_pt(tool_map)\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.tool_definitions_to_json-Tuple{Vector{ToolDefinition}}","page":"Home","title":"Logfire.tool_definitions_to_json","text":"tool_definitions_to_json(tools::Vector{ToolDefinition}) -> String\n\nSerialize tool definitions to JSON string for gen_ai.tool.definitions attribute.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.tracer","page":"Home","title":"Logfire.tracer","text":"tracer(name::String = \"logfire\") -> Tracer\n\nGet a tracer instance for creating spans.\n\n\n\n\n\n","category":"function"},{"location":"#Logfire.uninstrument_promptingtools!-Tuple{}","page":"Home","title":"Logfire.uninstrument_promptingtools!","text":"uninstrument_promptingtools!()\n\nBest-effort removal. Since PromptingTools does not expose a deregistration hook, this currently logs a warning and leaves registrations intact.\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.with_llm_span-Tuple{Any, String}","page":"Home","title":"Logfire.with_llm_span","text":"with_llm_span(f, operation::String; system=\"openai\", model=\"\", kwargs...)\n\nCreate a span for an LLM operation with GenAI semantic convention attributes.\n\nArguments\n\nf: Function to execute within the span\noperation: The operation name (e.g., \"chat\", \"embed\", \"completion\")\nsystem: The AI system/provider (e.g., \"openai\", \"anthropic\")\nmodel: The model name/ID\nkwargs: Additional attributes to set on the span\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.with_span-Tuple{Any, String}","page":"Home","title":"Logfire.with_span","text":"with_span(f, name::String; attrs...)\n\nCreate a span with the given name and execute the function within it. The function f receives the span as an argument.\n\nIf auto_record_exceptions is enabled in configuration (default: true), exceptions thrown within the span will be automatically recorded using OpenTelemetry semantic conventions before being rethrown.\n\nExample\n\n# With auto_record_exceptions enabled (default)\nLogfire.with_span(\"my-operation\") do span\n    error(\"This will be automatically recorded!\")  # Exception automatically captured\nend\n\n# Manual exception handling (still works)\nLogfire.with_span(\"my-operation\") do span\n    try\n        risky_operation()\n    catch e\n        Logfire.record_exception!(span, e)\n        # Handle exception...\n    end\nend\n\n\n\n\n\n","category":"method"},{"location":"#Logfire.wrap-Tuple{PromptingTools.AbstractPromptSchema}","page":"Home","title":"Logfire.wrap","text":"wrap(schema::PT.AbstractPromptSchema) -> LogfireSchema\n\nConvenience helper to wrap an existing PromptingTools schema.\n\n\n\n\n\n","category":"method"},{"location":"otel-genai/#OpenTelemetry-GenAI-Semantic-Conventions","page":"OTEL GenAI Semantic Conventions","title":"OpenTelemetry GenAI Semantic Conventions","text":"Logfire.jl implements the OpenTelemetry GenAI Semantic Conventions for tracing LLM operations, with specific adaptations for Logfire compatibility. This document describes the attributes and formats used.","category":"section"},{"location":"otel-genai/#Logfire-Specific-Deviations-from-OTEL-Standard","page":"OTEL GenAI Semantic Conventions","title":"Logfire-Specific Deviations from OTEL Standard","text":"While Logfire.jl follows the OTEL GenAI semantic conventions, it uses Logfire's message format which differs from the standard in several ways. These deviations ensure proper rendering in the Logfire UI.","category":"section"},{"location":"otel-genai/#Tool-Call-Responses","page":"OTEL GenAI Semantic Conventions","title":"Tool Call Responses","text":"The OTEL specification defines ToolCallResponsePart with a response field and role: \"tool\" for tool result messages. However, Logfire expects:\n\nAspect OTEL Standard Logfire Format\nField name response result\nMessage role tool user\nTool name Not specified name field included\n\nOTEL Standard format:\n\n{\n  \"role\": \"tool\",\n  \"parts\": [{\"type\": \"tool_call_response\", \"id\": \"call_123\", \"response\": \"22°C\"}]\n}\n\nLogfire format (what this library produces):\n\n{\n  \"role\": \"user\",\n  \"parts\": [{\"type\": \"tool_call_response\", \"id\": \"call_123\", \"name\": \"get_weather\", \"result\": \"22°C\"}]\n}","category":"section"},{"location":"otel-genai/#Why-These-Deviations?","page":"OTEL GenAI Semantic Conventions","title":"Why These Deviations?","text":"Logfire's UI has specific expectations for how tool results are displayed. Using the standard OTEL format results in tool responses being marked as \"Unrecognised\" in the Logfire dashboard. The Logfire format ensures:\n\nProper visualization - Tool results render correctly in the conversation view\nTool identification - The name field allows Logfire to associate results with their corresponding tool calls\nRole consistency - Using role: \"user\" matches how Logfire processes tool results internally","category":"section"},{"location":"otel-genai/#Reference-Specifications","page":"OTEL GenAI Semantic Conventions","title":"Reference Specifications","text":"OTEL GenAI Semantic Conventions: opentelemetry.io/docs/specs/semconv/gen-ai/\nOTEL GenAI Attributes Registry: opentelemetry.io/docs/specs/semconv/registry/attributes/gen-ai/\nLogfire Documentation: logfire.pydantic.dev/","category":"section"},{"location":"otel-genai/#References","page":"OTEL GenAI Semantic Conventions","title":"References","text":"GenAI Spans\nGenAI Agent Spans\nGenAI Events","category":"section"},{"location":"otel-genai/#Operation-Types","page":"OTEL GenAI Semantic Conventions","title":"Operation Types","text":"The gen_ai.operation.name attribute identifies the type of GenAI operation:\n\nValue Description Example\nchat Chat completion OpenAI Chat API, aigenerate\ncreate_agent Create GenAI agent Agent initialization\nembeddings Embeddings operation aiembed\nexecute_tool Execute a tool Tool execution spans\ngenerate_content Multimodal content generation Gemini Generate Content\ninvoke_agent Invoke GenAI agent Agent invocation\ntext_completion Text completions (legacy) Legacy completions API","category":"section"},{"location":"otel-genai/#Span-Attributes","page":"OTEL GenAI Semantic Conventions","title":"Span Attributes","text":"","category":"section"},{"location":"otel-genai/#Required-Attributes","page":"OTEL GenAI Semantic Conventions","title":"Required Attributes","text":"Attribute Type Description\ngen_ai.operation.name string Operation type (see above)\ngen_ai.provider.name string Provider identifier (e.g., \"openai\", \"anthropic\")","category":"section"},{"location":"otel-genai/#Request-Attributes","page":"OTEL GenAI Semantic Conventions","title":"Request Attributes","text":"Attribute Type Description\ngen_ai.request.model string Model requested (e.g., \"gpt-4o-mini\")\ngen_ai.request.temperature double Temperature setting\ngen_ai.request.max_tokens int Maximum tokens for response\ngen_ai.request.top_p double Top-p sampling setting\ngen_ai.request.frequency_penalty double Frequency penalty\ngen_ai.request.presence_penalty double Presence penalty\ngen_ai.request.stop_sequences string[] Stop sequences\ngen_ai.request.seed int Random seed if used","category":"section"},{"location":"otel-genai/#Response-Attributes","page":"OTEL GenAI Semantic Conventions","title":"Response Attributes","text":"Attribute Type Description\ngen_ai.response.model string Model that generated response\ngen_ai.response.id string Completion identifier\ngen_ai.response.finish_reasons string[] Why model stopped generating\ngen_ai.usage.input_tokens int Tokens in prompt\ngen_ai.usage.output_tokens int Tokens in response","category":"section"},{"location":"otel-genai/#Output-Type","page":"OTEL GenAI Semantic Conventions","title":"Output Type","text":"The gen_ai.output.type attribute describes the output format:\n\nValue Description\ntext Plain text\njson JSON object (known or unknown schema)\nimage Image\nspeech Speech","category":"section"},{"location":"otel-genai/#Message-Formats","page":"OTEL GenAI Semantic Conventions","title":"Message Formats","text":"Messages follow a parts-based format with the following structure.","category":"section"},{"location":"otel-genai/#Input-Messages-(gen_ai.input.messages)","page":"OTEL GenAI Semantic Conventions","title":"Input Messages (gen_ai.input.messages)","text":"Array of messages sent to the model:\n\n[\n  {\n    \"role\": \"user\",\n    \"parts\": [{\"type\": \"text\", \"content\": \"What's the weather?\"}]\n  },\n  {\n    \"role\": \"assistant\",\n    \"parts\": [\n      {\"type\": \"tool_call\", \"id\": \"call_123\", \"name\": \"get_weather\", \"arguments\": {\"city\": \"Paris\"}}\n    ]\n  },\n  {\n    \"role\": \"user\",\n    \"parts\": [\n      {\"type\": \"tool_call_response\", \"id\": \"call_123\", \"name\": \"get_weather\", \"result\": \"22°C, sunny\"}\n    ]\n  }\n]\n\nNote: Tool call responses use role: \"user\" and result field for Logfire compatibility. See Logfire-Specific Deviations for details.","category":"section"},{"location":"otel-genai/#Output-Messages-(gen_ai.output.messages)","page":"OTEL GenAI Semantic Conventions","title":"Output Messages (gen_ai.output.messages)","text":"Array of messages returned by the model (includes finish_reason):\n\n[\n  {\n    \"role\": \"assistant\",\n    \"parts\": [{\"type\": \"text\", \"content\": \"The weather in Paris is 22°C and sunny.\"}],\n    \"finish_reason\": \"stop\"\n  }\n]","category":"section"},{"location":"otel-genai/#System-Instructions-(gen_ai.system_instructions)","page":"OTEL GenAI Semantic Conventions","title":"System Instructions (gen_ai.system_instructions)","text":"System prompt separate from chat history:\n\n[\n  {\"type\": \"text\", \"content\": \"You are a helpful weather assistant.\"}\n]","category":"section"},{"location":"otel-genai/#Message-Roles","page":"OTEL GenAI Semantic Conventions","title":"Message Roles","text":"Role Description\nsystem System instructions\nuser User input (also used for tool execution results in Logfire format)\nassistant Model response\n\nNote: The OTEL standard defines a tool role, but Logfire expects tool results to use role: \"user\". See Logfire-Specific Deviations.","category":"section"},{"location":"otel-genai/#Message-Part-Types","page":"OTEL GenAI Semantic Conventions","title":"Message Part Types","text":"","category":"section"},{"location":"otel-genai/#TextPart","page":"OTEL GenAI Semantic Conventions","title":"TextPart","text":"Plain text content:\n\n{\"type\": \"text\", \"content\": \"Hello, world!\"}","category":"section"},{"location":"otel-genai/#ToolCallRequestPart","page":"OTEL GenAI Semantic Conventions","title":"ToolCallRequestPart","text":"Tool call requested by the model:\n\n{\n  \"type\": \"tool_call\",\n  \"id\": \"call_abc123\",\n  \"name\": \"get_weather\",\n  \"arguments\": {\"city\": \"Paris\", \"unit\": \"celsius\"}\n}","category":"section"},{"location":"otel-genai/#ToolCallResponsePart","page":"OTEL GenAI Semantic Conventions","title":"ToolCallResponsePart","text":"Tool execution result (Logfire format):\n\n{\n  \"type\": \"tool_call_response\",\n  \"id\": \"call_abc123\",\n  \"name\": \"get_weather\",\n  \"result\": \"22°C, sunny\"\n}\n\nNote: Uses result (not response) and includes name for Logfire compatibility.","category":"section"},{"location":"otel-genai/#BlobPart","page":"OTEL GenAI Semantic Conventions","title":"BlobPart","text":"Inline binary data (base64-encoded):\n\n{\n  \"type\": \"blob\",\n  \"modality\": \"image\",\n  \"mime_type\": \"image/png\",\n  \"content\": \"iVBORw0KGgoAAAANSUhEUgAA...\"\n}","category":"section"},{"location":"otel-genai/#UriPart","page":"OTEL GenAI Semantic Conventions","title":"UriPart","text":"External file by URI:\n\n{\n  \"type\": \"uri\",\n  \"modality\": \"image\",\n  \"uri\": \"https://example.com/image.png\"\n}","category":"section"},{"location":"otel-genai/#FilePart","page":"OTEL GenAI Semantic Conventions","title":"FilePart","text":"Pre-uploaded file by ID:\n\n{\n  \"type\": \"file\",\n  \"modality\": \"image\",\n  \"file_id\": \"file-abc123\"\n}","category":"section"},{"location":"otel-genai/#ReasoningPart","page":"OTEL GenAI Semantic Conventions","title":"ReasoningPart","text":"Model reasoning/thinking content:\n\n{\"type\": \"reasoning\", \"content\": \"Let me think about this...\"}","category":"section"},{"location":"otel-genai/#Finish-Reasons","page":"OTEL GenAI Semantic Conventions","title":"Finish Reasons","text":"Value Description\nstop Natural completion\nlength Max tokens reached\ncontent_filter Content filtered\ntool_call Model requested tool execution\nerror Error occurred","category":"section"},{"location":"otel-genai/#Tool-Definitions-(gen_ai.tool.definitions)","page":"OTEL GenAI Semantic Conventions","title":"Tool Definitions (gen_ai.tool.definitions)","text":"Array of available tools in OpenAI function format:\n\n[\n  {\n    \"type\": \"function\",\n    \"name\": \"get_weather\",\n    \"description\": \"Get current weather for a city\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"city\": {\"type\": \"string\", \"description\": \"City name\"},\n        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n      },\n      \"required\": [\"city\"]\n    }\n  }\n]","category":"section"},{"location":"otel-genai/#Error-Handling","page":"OTEL GenAI Semantic Conventions","title":"Error Handling","text":"When an error occurs, set error.type:\n\nValue Description\n_OTHER Fallback error value\nCustom Specific error type (e.g., \"ratelimit\", \"invalidapi_key\")","category":"section"},{"location":"otel-genai/#Julia-Types","page":"OTEL GenAI Semantic Conventions","title":"Julia Types","text":"Logfire.jl provides Julia types for all message formats in src/types.jl:\n\nusing Logfire: TextPart, ToolCallRequestPart, ToolCallResponsePart\nusing Logfire: InputMessage, OutputMessage\nusing Logfire: ROLE_USER, ROLE_ASSISTANT, FINISH_STOP\n\n# Create a text message\nmsg = InputMessage(ROLE_USER, [TextPart(\"Hello!\")])\n\n# Create a tool call response\nresponse = OutputMessage(\n    ROLE_ASSISTANT,\n    [TextPart(\"The weather is sunny.\")],\n    FINISH_STOP\n)","category":"section"},{"location":"otel-genai/#Usage-with-PromptingTools","page":"OTEL GenAI Semantic Conventions","title":"Usage with PromptingTools","text":"When using the LogfireSchema wrapper, messages are automatically converted:\n\nusing DotEnv\nDotEnv.load!()  # Load .env file (must call explicitly)\n\nusing Logfire\nusing PromptingTools\n\nLogfire.configure()\nLogfire.instrument_promptingtools!()\n\n# Messages are automatically traced with OTEL GenAI attributes\nresponse = aigenerate(\"What is 2+2?\")\n\nThe tracer extracts:\n\nSystem messages → gen_ai.system_instructions\nConversation history → gen_ai.input.messages\nModel response → gen_ai.output.messages\nTool definitions (if using aitools) → gen_ai.tool.definitions","category":"section"},{"location":"otel-genai/#Tool-Calls-Example","page":"OTEL GenAI Semantic Conventions","title":"Tool Calls Example","text":"using DotEnv\nDotEnv.load!()\n\nusing Logfire\nusing PromptingTools\nimport PromptingTools as PT\n\nLogfire.configure()\nLogfire.instrument_promptingtools!()\n\n# Define tools\n\"Get weather for a city\"\nget_weather(city::String) = \"22°C, sunny\"\n\n\"Get current time for a city\"\nget_time(city::String) = \"3:45 PM\"\n\ntools = [get_weather, get_time]\ntool_map = PT.tool_call_signature(tools)\n\n# Multi-turn conversation with tools\nconv = aitools(\"What's the weather in Paris?\"; tools, model=\"gpt4om\", return_all=true)\n\n# Execute tool calls\nif conv[end] isa PT.AIToolRequest\n    for tc in conv[end].tool_calls\n        tc.content = string(PT.execute_tool(tool_map, tc))\n        push!(conv, tc)\n    end\nend\n\n# Get final response\nresp = aigenerate(conv; model=\"gpt4om\")\npush!(conv, resp)","category":"section"},{"location":"otel-genai/#Julia-API-Reference","page":"OTEL GenAI Semantic Conventions","title":"Julia API Reference","text":"","category":"section"},{"location":"otel-genai/#Creating-Messages-Manually","page":"OTEL GenAI Semantic Conventions","title":"Creating Messages Manually","text":"using Logfire\n\n# Create a user message with text\nuser_msg = InputMessage(ROLE_USER, [TextPart(\"What's the weather?\")])\n\n# Create an assistant response with tool call\nassistant_msg = InputMessage(ROLE_ASSISTANT, [\n    ToolCallRequestPart(\"get_weather\"; id=\"call_123\", arguments=Dict(\"city\" => \"Paris\"))\n])\n\n# Create a tool response (uses ROLE_USER for Logfire compatibility)\ntool_msg = InputMessage(ROLE_USER, [\n    ToolCallResponsePart(\"22°C, sunny\"; id=\"call_123\", name=\"get_weather\")\n])\n\n# Create output message with finish reason\noutput = OutputMessage(ROLE_ASSISTANT, [TextPart(\"The weather is sunny.\")], FINISH_STOP)\n\n# Serialize to JSON\njson_input = messages_to_json([user_msg, assistant_msg, tool_msg])\njson_output = messages_to_json([output])","category":"section"},{"location":"otel-genai/#Creating-Tool-Definitions","page":"OTEL GenAI Semantic Conventions","title":"Creating Tool Definitions","text":"using Logfire\n\n# Create tool definition manually\ntool = ToolDefinition(\n    \"get_weather\";\n    description=\"Get current weather for a city\",\n    parameters=Dict{String,Any}(\n        \"type\" => \"object\",\n        \"properties\" => Dict(\n            \"city\" => Dict(\"type\" => \"string\", \"description\" => \"City name\"),\n            \"unit\" => Dict(\"type\" => \"string\", \"enum\" => [\"celsius\", \"fahrenheit\"])\n        ),\n        \"required\" => [\"city\"]\n    )\n)\n\n# Serialize to JSON\njson = tool_definitions_to_json([tool])","category":"section"},{"location":"otel-genai/#Converting-PromptingTools-Messages","page":"OTEL GenAI Semantic Conventions","title":"Converting PromptingTools Messages","text":"using Logfire\nusing PromptingTools as PT\n\n# Convert a PT conversation to OTEL format\nconv = [\n    PT.SystemMessage(\"You are helpful\"),\n    PT.UserMessage(\"Hello\"),\n    PT.AIMessage(\"Hi there!\")\n]\n\n# Convert with system message extraction\nresult = pt_conversation_to_otel(conv; separate_system=true)\n\n# Access converted messages\nprintln(result.system_instructions)  # Array of TextPart\nprintln(result.input_messages)       # Array of InputMessage\nprintln(result.output_messages)      # Array of OutputMessage","category":"section"},{"location":"otel-genai/#Setting-Span-Attributes","page":"OTEL GenAI Semantic Conventions","title":"Setting Span Attributes","text":"using Logfire\nusing OpenTelemetryAPI\n\n# Create a span\nspan = create_span(\"gen_ai.chat\", tracer(\"myapp\"))\n\n# Set messages from a PT conversation\nset_genai_messages!(span, conv)\n\n# Set tool definitions\nset_tool_definitions!(span, tool_map)\n\n# End span\nend_span!(span)","category":"section"}]
}
